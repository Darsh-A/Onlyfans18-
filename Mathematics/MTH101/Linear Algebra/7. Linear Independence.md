
## Linear Independence

A set $S \subset V$ is said to be ***Linearly Independent*** if for any $v \in S$ , we have ,  $\color{#d693f5}v \notin Span(S \setminus \{v\})$

This means that no element can be written as a linear combination of other elements.
No element can be thrown out without altering the span.


### Characterization of Linear Independence

 Let $V$ be a vector space
 Let $S \subset V$ . Then,
 1. $S$ is linearly Independent
 2. Any element of $Span(S)$ can be uniquely written in the form $\sum_{v \in S}a_{v} \cdot v$
 3. If $\sum_{v \in S}a_{v} \cdot v = \bar{0}$  ,  then we must have $a_{v} = 0 \:\: \forall \:\: v$  
    There is no collection of real numbers $\{a_{v}\}_{v \in S}$ such that at least one of them is non-zero and $\sum a_{v}\cdot v =0$
    
    ![](https://i.imgur.com/wzxskAw.gif)


#### Proof that 1. implies 3.

We assume $S$ is linearly independent.
Suppose $\sum_{v \in S} a_{v} \cdot v =0$  for some choice of real numbers $a_{v}$.
We want to prove that $a_{v}= 0$ for all $v$
Suppose, for some $v_{o} \in S$  ,  $a_{v_{o}} \neq 0$
Then,
	$a_{v_{o}} \cdot v + \sum_{v \neq v_{o}} a_{v} \cdot v = 0$
So,
	$v_{o} = \sum _{v \neq v_{o}}\left( \frac{-a_{v}}{a_{v_{o}}} \right) \cdot v$
Thus,
	$v_{o} \in Span(S \setminus \{v_{o}\})$        $-$ **contradiction**
Thus,
	$a_{v}=0 \:\: \forall \:\: v \in S$

#### Proof that 3. implies 1.
We assume $3.$ is true.
Suppose $S$ is not linearly independent
Thus, for some $v \in S$ , we have $v \in Span(S \setminus \{v\})$ .
So,
	$v = \sum_{w \neq v}a_{w} \cdot w$
So,
	$-v + \sum_{w \neq v}a_{w}\cdot w = \bar{0}$
In this expression, the coefficient of $v$ is $-1$ , which is non-zero
This **contradicts** our assumption that $3.$ is true
Thus $S$ must be linearly independent.


#### Proof of 3. implies 2.

Assume $3.$ is true.
If $2.$ is not true, some $v \in Span(S)$ has two distinct expressions.
$$
v = \sum_{v \in S}a_{w} \cdot w \quad \text{and} \quad v = \sum_{w \in S} b_{w} \cdot w
$$
So,
	$\bar{0} = \sum_{w \in S}(a_{w}-b_{w}) \cdot w$
As the two expressions are distinct, there exists some $w_{o} \in S$ such that $a_{w_{o}} \neq b_{w_{o}}$
So,
	$a_{w_{o}}-b_{w_{o}} \neq 0$
As
	$\sum_{w \in S}(a_{w}-b_{w}) \cdot w = 0$  , this shows that $3.$ isn't true  $-$ **contradiction**


#### Proof that 2. implies 3.

Assume $3.$ is not true.
So, there exists an equality  $\sum_{w \in S}a_{w} \cdot w = \bar{0}$
where not all $a_{w}$ are equal to $0$
But, we also have $\sum_{w \in S} 0 \cdot w = \bar{0}$
	This violates $2.$ ( for $v = \bar{0}$ )      $-$ **contradiction**

So, we have proved
$1. \iff 2. \iff 3.$
Thus the conditions are equivalent

##### Note:
If all $a_{v}=0$ , then we say that it is a ***Trivial Linear Relation***.
If not then it is a ***Non-Trivial Linear Relation*** between the elements of $S$.

```ad-note
title: Linearly Dependent
A set is linearly independent if and only if there is no non-trivial linear relation between its element.

If a set is not linearly independent, we say that it is linearly dependent.

```


---

## Detecting Linear Dependence/Independence

In general we try to find a non-trivial solution to the equation
$$\sum_{w \in S} x_{w}\cdot w = 0$$
where $x_{w}$ are the variables
When $S$ is finite, say $S = \{v_{1}\dots v_{m}\}$ this means we have to find solutions to $x_{1}v_{1}+\dots x_{m}v_{m}=\bar{0}$

Suppose,
$S = \{v_{1}\dots v_{m}\} \subseteq \mathbb{R}^{n}$
If they are linearly dependent, we want to find $x_{1},\dots x_{m} \in \mathbb{R}$ such that ,
$$
\sum_{i=1}^{m}x_{i}v_{i} = \bar{0} \qquad \text{for some}\:\:\:x_{i} \neq 0
$$
Let $A \in M_{n\text{x}m}(\mathbb{R})$ be the matrix whose columns are $v_{1},\dots ,v_{m}$ .
Let $$
X = \begin{pmatrix}
x_{1} \\
\vdots \\
x_{m}
\end{pmatrix}
$$
$\color{#d693f5}\text{Then, we want to solve the matrix equation}\: AX = \bar{0}$
$\color{#d693f5} \text{and see if it has a non-zero solution}$


##### Example:
Determine if the set is linearly independent
$$
S = \left\{\begin{pmatrix}
1 \\
-1 \\
2
\end{pmatrix},
\begin{pmatrix}
2 \\
2 \\
5
\end{pmatrix},
\begin{pmatrix}
0 \\
4 \\
1
\end{pmatrix}
\right\}
$$
###### Solution
We want to find solutions to
$$
x_{1} \cdot \begin{pmatrix}
1 \\
-1 \\
2
\end{pmatrix}+
x_{2} \cdot \begin{pmatrix}
2 \\
2 \\
5 \\
\end{pmatrix}+
x_{3} \cdot \begin{pmatrix}
0 \\
4 \\
1
\end{pmatrix} = \begin{pmatrix}
0 \\
0 \\
0
\end{pmatrix}
$$
This is represented by
$$
\left(\begin{array}{ccc|c}
1 & 2 & 0 & 0 \\
-1 & 2 & 4 & 0 \\
2 & 5 & 1 & 0
\end{array}\right)
$$
We solve as usual,
$$
\begin{align}
\left(\begin{array}{ccc|c}
1 & 2 & 0 & 0 \\
-1 & 2 & 4 & 0 \\
2 & 5 & 1 & 0
\end{array}\right) 
&\xrightarrow {R_{2}+R_{1}\:;\:R_{3}-2R_{1}} \left(\begin{array}{ccc|c}
\color{#fc6f86}1 & 2 & 0 & 0 \\
0 & 4 & 4 & 0 \\
0 & 1 & 1 & 0
\end{array}\right) \\ \\
\xrightarrow {\frac{1}{4}R_{2}} \left(\begin{array}{ccc|c}
\color{#fc6f86}1 & 2 & 0 & 0 \\
0 & 1 & 1 & 0 \\
0 & 1 & 1 & 0
\end{array}\right)
&\xrightarrow {R_{1}-R_{2}\:;\:R_{3}-R_{2}}
\left(\begin{array}{ccc|c}
\color{#fc6f86}1 & 0 & -2 & 0 \\
0 & \color{#fc6f86}1 & 1 & 0 \\
0 & 0 & 0 & 0
\end{array}\right) \\ \\
\text{Solution Set}\:\: &= \left\{\begin{pmatrix}
2t \\
-t \\
t
\end{pmatrix} \:\:\Bigg| \:\:t \in \mathbb{R}
\right\}
\end{align}
$$
	Taking $t=1$, we get $x_{1}=2,x_{2}=-1,x_{3}=1$

So, the set is not linearly independent. We have the non-trivial linear relation
$$
2 \cdot \begin{pmatrix}
1 \\
-1 \\
2
\end{pmatrix}
-
1\cdot \begin{pmatrix}
2 \\
2 \\
5
\end{pmatrix}
+
1 \cdot \begin{pmatrix}
0 \\
4 \\
1
\end{pmatrix}
=
\begin{pmatrix}
0 \\
0 \\
0
\end{pmatrix}
$$

Cramer's Rule cant be used as
$$
\det \begin{pmatrix}
1 & 2 & 0 \\
-1 & 2 & 4 \\
2 & 5 & 1
\end{pmatrix} = 0
$$
***Since, this is a [[3. Vector Spaces#$ mathbb{R} {n}$ : Vector Space |Homogeneous System]], it always has a solution.***
So, it will have infinite solutions.

---

### The Determinant method

##### Column Rank
Given a matrix, the dimension of the $Span$ of the columns is called the **Column Rank** of the matrix.

##### $k\text{x}k\:\: minor$ 
Let $A \in M_{m\text{x}n}(\mathbb{R})$ 
Let $k \leq min(m,n)$
Suppose we pick any $k-rows$ and any $k-columns$ . They meet in $k$x$k$ positions. The $k$x$k$ matrix formed like this is called $\color{#d693f5}k\text{x}k\:\:minor$ of $A$.


#### Theorem 7.1
Suppose some $\color{#d693f5}k\text{x}k\:\:minor$ of $A$ has non-zero determinant . Then $\color{#d693f5}column-rank$ of $A$ is $\geq k$ 

#### Theorem 7.2
$Colm.rank(A) = Row.rank(A)$
Thus we can just say $Rank(A)$


##### The Method:

Given any $m$ vectors in $\mathbb{R}^{n}$ 

- Take the $n$x$m$ matrix whose columns are the given vectors.
  If there exists an $\color{#d693f5}m\text{x}m\:\: minor$ with non-zero determinant, the vectors are ***Linearly Independent***

- If ***all*** $\color{#d693f5}m\text{x}m\:\: minor$  have determinant $0$ , the vectors are ***Linearly Dependent***
	  So, if $\color{#fc6f86}m>n$ , as there are no $\color{#d693f5}m\text{x}m\:\: minor$ at all, the vectors are *Linearly Dependent*. 









