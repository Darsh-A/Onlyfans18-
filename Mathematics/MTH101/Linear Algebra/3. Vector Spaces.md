





# .

$M_{n\text{x}1}(\mathbb{R})$ : column vectors with $n$ entries.
$M_{1\text{x}n}(\mathbb{R})$ : row vectors with $n$ entries

## $\mathbb{R}^{n}$ : Vector Space
>  - It has a binary operation **Addition** where it is an [[8.Subset of Subgroups#Abelian Groups | Abelian Group]] 
> -  It has a operation called **Scaler Multiplication** which is a function $\mathbb{R}$x$\mathbb{R}^{n}\rightarrow \mathbb{R}^{n}$
> $$
\begin{pmatrix}
\alpha \:\:\:, & \begin{pmatrix}
x_{1} \\
x_{2} \\
\vdots  \\
x_{n}
\end{pmatrix}
\end{pmatrix}
\longmapsto
\begin{pmatrix}
\alpha x_{1} \\
\alpha x_{2} \\
\vdots \\
\alpha x_{n}
\end{pmatrix}
$$
>   (***note***: $\longmapsto$ means maps to)
> 	  ***This is NOT a binary operation on $\mathbb{R}^{n}$***
> 	

```ad-note
Intuitively we can understand $\mathbb{R}^{n}$ as just a set of vectors (which can be thought of a multidimensional space)
```


---


## Vector Space

A (real) vector space, or a vector space over $\mathbb{R}$ is a triple ( $V$, +, $\cdot$ )  where,
- $V$ is a set
- $+$ is a binary operation on $V$
- $\cdot$ is a function $\mathbb{R}$x$V \rightarrow V$ , such that
  1. $V$ is an [[8.Subset of Subgroups#Abelian Groups | Abelian Group]] under $+$
  2. $1.v=v$ $\forall$ $v\in V$ 
  3. $(\alpha + \beta)\cdot v = \alpha v + \beta v \:\: \forall \:\: \alpha , \beta \in \mathbb{R}$  and  $\forall\:\: v \in V$
  4. $\alpha \cdot (v_{1}+v_{2}) = \alpha v_{1} + \alpha v_{2} \:\: \forall \:\: \alpha \in \mathbb{R}$ and $\forall \:\: v_{1}, v_{2} \in V$

###### Note:
> We may replace $\mathbb{R}$ by $\mathbb{C}$ or $\mathbb{Q}$ respectively.
> More generally we can define vector spaces over any *field*. 
> 	Fields are sets with two binary operations $+$ and x satisfying **certain properties**.

When we study vector spaces, we study functions $V_{1}\rightarrow V_{2}$ which *respects* **Addition** and **Scaler Multiplication**

#### Basic Properties

1. $0 \cdot v = \bar{0} \:\:\:\forall \:\:v \in \mathbb{R}$
   where $0$ is the $0$ in $\mathbb{R}$  and  $\bar{0}$ is the identity for $+$ in $V$
   ***Proof***:
   >$0 \cdot v = (0+0)v = 0 \cdot v + 0 \cdot v$
   >So, $0 \cdot v = \bar{0}$

2. $-v = (-1) \cdot v \:\:\: \forall \:\: v \in \mathbb{R}$
   ***Proof:***
   > $v+(-1) \cdot v = 1 \cdot v + (-1) \cdot v = (1+(-1)) \cdot v = 0 \cdot v = \bar{0}$
   > Thus,
   > 	$(-1)\cdot v = -v$

#### Example of Vector Spaces:

1. Let $S$ be any set. Let $F$ be the set of functions from $S$ to $\mathbb{R}$ 
   For functions $f,g \in F$, we define the function $f+g$ by the formula 
   $(f+g)(s) = f(s)+g(s)\:\:\: \forall \:\: s \in S$
   $F$ is an Abelian group under $+$ 
   For $\alpha \in \mathbb{R}$ and $f \in F$ , we define the function $\alpha f$ by 
   $(\alpha f) (s) = \alpha \cdot f(s)$
   $F$ is a vector space with these Operations

2. Let $\mathbb{R}[X]$ denote the set of polynomials in the variable $X$ , with coefficients in $\mathbb{R}$ with the usual addition operation on polynomials, $\mathbb{R}[X]$ is an Abelian Group. 
    If $\alpha \in \mathbb{R}$ and $p$ is the polynomial $a_{0}+a_{1}X+\dots + a_{n}X^{n}$  we define $\alpha p$ to be $\alpha a_{0}+(\alpha a_{1})X + \dots + (\alpha a_{n})X^{n}$.
   This gives $\mathbb{R}[X]$ the **structure** of a vector space

3. Let $m$ and $n$ be positive integers.
   Suppose we have a system of $m$ equations in $n$ variable as follows:
$$

^fdb3d3

\begin{align}
a_{11}X_{1} + a_{12}X_{2} + \dots + a_{1n}X_{n} &= 0 \\
a_{21}X_{1} + a_{22}X_{2} + \dots + a_{2n}X_{n} &= 0 \\
\vdots \quad\qquad\qquad\qquad\qquad\qquad & \quad\:\vdots \\
a_{m1}X_{1} + a_{m2}X_{2} + \dots + a_{mn}X_{n} &= 0
\end{align} 
$$
Notice that all constant terms are zero.
Such a system is called a ***Homogeneous System.***
Recall that we write this system as the matrix operation $AX = 0$
A solution is an element $v \in \mathbb{R}^{n}$ such that $Av = 0$ .
If $v_{1}$ and $v_{2}$ are solutions, then $A(v_{1}+v_{2}) = Av_{1}+Av_{2} = 0$
Thus, $v_{1}+v_{2}$ is a solution as well.
Similarly, if $\alpha \in \mathbb{R}$ and $v$ is a solution , then $A(\alpha v) = 0$.
	if,
$$
v = \begin{pmatrix}
x_{1} \\
\vdots \\
x_{n}
\end{pmatrix} \quad , \quad
\alpha v = \begin{pmatrix}
\alpha x_{1} \\
\vdots \\
\alpha x_{n}
\end{pmatrix}
$$
As $a_{11} x_{1}+ \dots a_{1n}x_{n} = 0$
we have $a_{11}(\alpha x_{1})+\dots + a_{1n}(\alpha x_{n}) = 0$
It is easy to check from this that the set of solutions is a vector space in which addition and scalar multiplication are given by the usual operations on $\mathbb{R}^{n}$.
Thus, it is an example of a ***SUBSPACE*** of $\mathbb{R}^{n}$.

---

## Vector Subspace 

Let $V$ be a vector space. A Subspace $W$ of $V$ is a subset $W \subset V$ such that it is a vector space under operations **addition** and **scalar multiplication**. 

Thus $W$ must at least have these conditions:
1. If $w_{1},w_{2} \in W$ , then $w_{1}+w_{2} \in W$     $-$ closed under addition
2. If $w \in W$ and $\alpha \in \mathbb{R}$ , then $\alpha w \in V$    $-$ closed under scalar multiplication

##### Existence of $\bar{0}$
Suppose $W$ is non-empty. Take any $w \in W$ . As $0 \in \mathbb{R}$ , $0 \cdot w \in W$ , i.e, $\bar{0} \in W$

##### Existence of Inverses:
If $w \in W$,    $(-1) \cdot w \in W$ 
But, $(-1) \cdot w = -w$
So, $-1 \in W$.

##### Examples:

1. Let $V$ be a vector space. Then, $V$ is a subspace of $V$. Also, $\{\bar{0}\}$ is a subspace of $V$.
   (This is the "**zero Subspace**")

2. As we saw the solutions of a **Homogeneous** system in $n$ variables is a subspace of $\mathbb{R}^{n}$

***Taking ideas from Group Theory to get analogous results about subspaces.***

1. The intersection of any family of subspaces of $V$ is a subspace of $V.$
2. Let $V$ be a vector space and $S \subset V$ be a subset. 
   The intersection of all subspaces of $V$ containing $S$ is a subspace of $V$  (by 1.)
   This is called the ***SPAN*** of $S$ and is written as $Span(S)$.
   This is analogous to the notion of subgroup generated by a set.

---

## Span

$Span(s) = \{\:a_{1}v_{1}+\dots+a_{n}v_{n}\:\:\: |\:\: v_{1},\dots v_{n} \in S \: ;\: a_{1}\dots a_{n} \in \mathbb{R}\: \}$
$i.e,$
The span of most pairs of vectors ends up being the whole 2-D or $\mathbb{R}^{2}$ space

![](https://i.imgur.com/1yQ4v3e.gif)

The elements of $Span(S)$ are called the **Linear Combinations of S**

###### Note:
If the pair of vectors are collinear to each other then the $Span$ comes out to be a straight line.
Thus if we can obtain the span even after removing a vector we call these vectors as ***LINEARLY DEPENDENT***
$i.e,$ one of the vectors can be expressed as a linear combination of the other

![](https://i.imgur.com/zq0mh6L.gif)


#### Examples:

Let $V = \mathbb{R}^{3}$  and let $v_{1}, v_{2} \in \mathbb{R}$.
$Span(\{v_{1},v_{2}\}) = \{\:\:a_{1}v_{1}+a_{2}v_{2}\:\:|\:\:a_{1},a_{2} \in \mathbb{R} \:\:\}$

- If $v_{1}=v_{2}=0$,   $Span(\{v_{1},v_{2}\}) = \{0\}$

- If $v_{1} \neq 0$  , and $v_{2}=\alpha v_{1}$ for some $\alpha \in \mathbb{R}$ , then $a_{1}v_{1}+a_{2}v_{2}$ is just $(a_{1}+a_{2}\alpha)v_{1}\in Span(\{v_{1}\})$
  Thus, in this case, $Span(\{v_{1},v_{2}\})$ is the line through $0$ and $v_{1}$.

- If $v_{1} \neq {0}$ and $v_{2} \neq 0$ and $v_{1} \neq \alpha v_{2}$ for any $\alpha$
  Then, the $Span(\{v_{1},v_{2}\})$ can be shown to be the plane containing $\bar{0},v_{1}$ and $v_{2}$
  
```ad-note
title: Definition
Let $V$ be a vector space

A subset $S \subset V$ is a **Spanning** set of $V$ if 

$V = Span(S)$
```

***Note:***
> If $S_{1} \subset S_{2}$ , then $Span(S_{1}) \subset Span(S_{2})$ . 
> So, if $S$ is a spanning set of $V$, Then $S$ is an subset of $V$ which contains $S$


#### Geometrical Intuition for Span

Collection of all locations in space that can be reached through movements in directions provided by elements of $S$.

More precisely the $Span$ of two vectors in $\mathbb{R}^{3}$ is the plane containing all the ***linear combinations*** of those two vectors

![](https://i.imgur.com/z7Q9PUx.gif)

And if we add another vector $i.e$, calculating the $Span$ of 3 vectors in $\mathbb{R}^{3}$ we can have two possibilities
either the third vector be linearly dependent and the $Span$ wont change
or it is ***linearly independent*** and then the $Span$ becomes the entire $\mathbb{R}^{3}$ Plane.

![](https://i.imgur.com/w8BPpJB.gif)


##### Theorem 3.1
Let $V$ be a vector space and let $S \subset V$
Let $v \in V$ 
$Span(S \:\cup \: \{v\}) \neq Span(S) \iff v \notin Span(S)$
( $\iff$ : if and only if )

###### ***Proof:***
>Let $V$ be a vector space and let $S \subset V$ .
>Let $v \in V$ . We know that
>	$Span(S) \subseteq Span(S\: \cup \: \{v\})$
>	
>$Span(S\:\cup \:\{v\})$ strictly bigger than $Span(S)$ when:
>	$v \in Span(S \:\cup \: \{v\})$
>	if $v \notin Span(S)$  ,  then  $Span(S \: \cup \: \{v\}) \neq Span(S)$
>
> If $Span(S \: \cup \: \{v\})$ is strictly bigger than $Span(S)$ then:
> 	we claim that $v \notin Span(S)$
> 	So,     $v = \sum_{w \in S}a_{w}\cdot w$     ,    $a_{w} \in \mathbb{R} \:\: \forall \:\: w$
> 	Any element $x \in Span(S \: \cup \: \{v\})$ is of the form 
> 		$x = b_{v} \cdot v + \sum_{w \in S} b_{w}\cdot w$
> 	So,
> 		$x=\sum_{w \in S}(b_{v}a_{w}+b_{w}) \cdot w$
> 	Thus,
> 		$x \in Span(S)$
> 	Thus,
> 		$Span(S \: \cup \: \{v\}) \subset Span(S)$        $-$ **contradiction**
> 	So,
> 	$v \notin Span(S)$


---



