$M_{n\text{x}1}(\mathbb{R})$ : column vectors with $n$ entries.
$M_{1\text{x}n}(\mathbb{R})$ : row vectors with $n$ entries

##### $\mathbb{R}^{n}$ : Vector Space
>  - It has a binary operation **Addition** where it is an [[8.Subset of Subgroups#Abelian Groups | Abelian Group]] 
> -  It has a operation called **Scaler Multiplication** which is a function $\mathbb{R}$x$\mathbb{R}^{n}\rightarrow \mathbb{R}^{n}$
> $$
\begin{pmatrix}
\alpha \:\:\:, & \begin{pmatrix}
x_{1} \\
x_{2} \\
\vdots  \\
x_{n}
\end{pmatrix}
\end{pmatrix}
\longmapsto
\begin{pmatrix}
\alpha x_{1} \\
\alpha x_{2} \\
\vdots \\
\alpha x_{n}
\end{pmatrix}
$$
>   (***note***: $\longmapsto$ means maps to)
> 	  ***This is NOT a binary operation on $\mathbb{R}^{n}$***
> 	  
```ad-note
Intuitively we can understand $\mathbb{R}^{n}$ as just a set of vectors (which can be thought of a multidimensional space)
```

---

## Vector Space

A (real) vector space, or a vector space over $\mathbb{R}$ is a triple ( $V$, +, $\cdot$ )  where,
- $V$ is a set
- $+$ is a binary operation on $V$
- $\cdot$ is a function $\mathbb{R}$x$V \rightarrow V$ , such that
  1. $V$ is an [[8.Subset of Subgroups#Abelian Groups | Abelian Group]] under $+$
  2. $1.v=v$ $\forall$ $v\in V$ 
  3. $(\alpha + \beta)\cdot v = \alpha v + \beta v \:\: \forall \:\: \alpha , \beta \in \mathbb{R}$  and  $\forall\:\: v \in V$
  4. $\alpha \cdot (v_{1}+v_{2}) = \alpha v_{1} + \alpha v_{2} \:\: \forall \:\: \alpha \in \mathbb{R}$ and $\forall \:\: v_{1}, v_{2} \in V$

###### Note:
> We may replace $\mathbb{R}$ by $\mathbb{C}$ or $\mathbb{Q}$ respectively.
> More generally we can define vector spaces over any *field*. 
> 	Fields are sets with two binary operations $+$ and x satisfying **certain properties**.

When we study vector spaces, we study functions $V_{1}\rightarrow V_{2}$ which *respects* **Addition** and **Scaler Multiplication**

#### Basic Properties

1. $0 \cdot v = \bar{0} \:\:\:\forall \:\:v \in \mathbb{R}$
   where $0$ is the $0$ in $\mathbb{R}$  and  $\bar{0}$ is the identity for $+$ in $V$
   ***Proof***:
   >$0 \cdot v = (0+0)v = 0 \cdot v + 0 \cdot v$
   >So, $0 \cdot v = \bar{0}$

2. $-v = (-1) \cdot v \:\:\: \forall \:\: v \in \mathbb{R}$
   ***Proof:***
   > $v+(-1) \cdot v = 1 \cdot v + (-1) \cdot v = (1+(-1)) \cdot v = 0 \cdot v = \bar{0}$
   > Thus,
   > 	$(-1)\cdot v = -v$

#### Example of Vector Spaces:

1. Let $S$ be any set. Let $F$ be the set of functions from $S$ to $\mathbb{R}$ 
   For functions $f,g \in F$, we define the function $f+g$ by the formula 
   $(f+g)(s) = f(s)+g(s)\:\:\: \forall \:\: s \in S$
   $F$ is an Abelian group under $+$ 
   For $\alpha \in \mathbb{R}$ and $f \in F$ , we define the function $\alpha f$ by 
   $(\alpha f) (s) = \alpha \cdot f(s)$
   $F$ is a vector space with these Operations

2. Let $\mathbb{R}[X]$ denote the set of polynomials in the variable $X$ , with coefficients in $\mathbb{R}$ with the usual addition operation on polynomials, $\mathbb{R}[X]$ is an Abelian Group. 
    If $\alpha \in \mathbb{R}$ and $p$ is the polynomial $a_{0}+a_{1}X+\dots + a_{n}X^{n}$  we define $\alpha p$ to be $\alpha a_{0}+(\alpha a_{1})X + \dots + (\alpha a_{n})X^{n}$.
   This gives $\mathbb{R}[X]$ the **structure** of a vector space

3. Let $m$ and $n$ be positive integers.
   Suppose we have a system of $m$ equations in $n$ variable as follows:
$$
\begin{align}
a_{11}X_{1} + a_{12}X_{2} + \dots + a_{1n}X_{n} &= 0 \\
a_{21}X_{1} + a_{22}X_{2} + \dots + a_{2n}X_{n} &= 0 \\
\vdots \quad\qquad\qquad\qquad\qquad\qquad & \quad\:\vdots \\
a_{m1}X_{1} + a_{m2}X_{2} + \dots + a_{mn}X_{n} &= 0
\end{align} 
$$
Notice that all constant terms are zero.
Such a system is called a ***Homogeneous System.***
Recall that we write this system as the matrix operation $AX = 0$
A solution is an element $v \in \mathbb{R}^{n}$ such that $Av = 0$ .
If $v_{1}$ and $v_{2}$ are solutions, then $A(v_{1}+v_{2}) = Av_{1}+Av_{2} = 0$
Thus, $v_{1}+v_{2}$ is a solution as well.
Similarly, if $\alpha \in \mathbb{R}$ and $v$ is a solution , then $A(\alpha v) = 0$.
	if,
$$
v = \begin{pmatrix}
x_{1} \\
\vdots \\
x_{n}
\end{pmatrix} \quad , \quad
\alpha v = \begin{pmatrix}
\alpha x_{1} \\
\vdots \\
\alpha x_{n}
\end{pmatrix}
$$
As $a_{11} x_{1}+ \dots a_{1n}x_{n} = 0$
we have $a_{11}(\alpha x_{1})+\dots + a_{1n}(\alpha x_{n}) = 0$
It is easy to check from this that the set of solutions is a vector space in which addition and scalar multiplication are given by the usual operations on $\mathbb{R}^{n}$.
Thus, it is an example of a ***SUBSPACE*** of $\mathbb{R}^{n}$.

---

## Vector Subspace 

Let $V$ be a vector space. A Subspace $W$ of $V$ is a subset $W \subset V$ such that it is a vector space under operations **addition** and **scalar multiplication**. 

Thus $W$ must at least have these conditions:
1. If $w_{1},w_{2} \in W$ , then $w_{1}+w_{2} \in W$     $-$ closed under addition
2. If $w \in W$ and $\alpha \in \mathbb{R}$ , then $\alpha w \in V$    $-$ closed under scalar multiplication

##### Existence of $\bar{0}$
Suppose $W$ is non-empty. Take any $w \in W$ . As $0 \in \mathbb{R}$ , $0 \cdot w \in W$ , i.e, $\bar{0} \in W$

##### Existence of Inverses:
If $w \in W$,    $(-1) \cdot w \in W$ 
But, $(-1) \cdot w = -w$
So, $-1 \in W$.

##### Examples:

1. Let $V$ be a vector space. Then, $V$ is a subspace of $V$. Also, $\{\bar{0}\}$ is a subspace of $V$.
   (This is the "**zero Subspace**")

2. As we saw the solutions of a **Homogeneous** system in $n$ variables is a subspace of $\mathbb{R}^{n}$

***Taking ideas from Group Theory to get analogous results about subspaces.***

1. The intersection of any family of subspaces of $V$ is a subspace of $V.$
2. Let $V$ be a vector space and $S \subset V$ be a subset. 
   The intersection of all subspaces of $V$ containing $S$ is a subspace of $V$  (by 1.)
   This is called the ***SPAN*** of $S$ and is written as $Span(S)$.
   This is analogous to the notion of subgroup generated by a set.


### Span

$Span(s) = \{\:a_{1}v_{1}+\dots+a_{n}v_{n}\:\:\: |\:\: v_{1},\dots v_{n} \in S \: ;\: a_{1}\dots a_{n} \in \mathbb{R}\: \}$
$i.e,$
The span of most pairs of vectors ends up being the whole 2-D or $\mathbb{R}^{2}$ space

![](https://i.imgur.com/1yQ4v3e.gif)

The elements of $Span(S)$ are called the **Linear Combinations of S**

###### Note:
If the pair of vectors are collinear to each other then the $Span$ comes out to be a straight line.

![](https://i.imgur.com/zq0mh6L.gif)


#### Examples:

Let $V = \mathbb{R}^{3}$  and let $v_{1}, v_{2} \in \mathbb{R}$.
$Span(\{v_{1},v_{2}\}) = \{\:\:a_{1}v_{1}+a_{2}v_{2}\:\:|\:\:a_{1},a_{2} \in \mathbb{R} \:\:\}$

- If $v_{1}=v_{2}=0$,   $Span(\{v_{1},v_{2}\}) = \{0\}$

- If $v_{1} \neq 0$  , and $v_{2}=\alpha v_{1}$ for some $\alpha \in \mathbb{R}$ , then $a_{1}v_{1}+a_{2}v_{2}$ is just $(a_{1}+a_{2}\alpha)v_{1}\in Span(\{v_{1}\})$
  Thus, in this case, $Span(\{v_{1},v_{2}\})$ is the line through $0$ and $v_{1}$.

- If $v_{1} \neq {0}$ and $v_{2} \neq 0$ and $v_{1} \neq \alpha v_{2}$ for any $\alpha$
  Then, the $Span(\{v_{1},v_{2}\})$ can be shown to be the plane containing $\bar{0},v_{1}$ and $v_{2}$
  

---

### Linear Transformation

Let $V_{1}$ and $V_{2}$ be vector spaces. A  ***Linear Map*** or a ***Linear Transformation*** is a function $\phi : V_{1} \rightarrow V_{2}$ such that the following two conditions hold
	1. $\phi$ is a group homomorphism with respect to addition
	2. $\phi(\alpha \cdot v) = \alpha \cdot \phi(v)$ for all $\alpha \in \mathbb{R}$ and $v \in V$

##### Example
Let $A$ be an $m$x$n$ matrix. Then, the function $\phi : \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ defined by $\phi(v) = A \cdot v$ is linear transformation

---

### Concept of Linearity

An $n$x$n$ matrix $A$ can be viewed as an **Ordered** collection of $n$ column vectors (*an "n-tuple" of column vectors*) 
$$
\begin{pmatrix}
\color{#fc6f86} a_{11} & \color{#93f5eb} a_{12}  &  \dots  &  \color{#d693f5} a_{1n} \\
\color{#fc6f86} a_{21} & \color{#93f5eb} a_{22}  &  \dots  &  \color{#d693f5} a_{2n} \\
\color{#fc6f86} \vdots & \color{#93f5eb} \vdots  & \dots  & \color{#d693f5}\vdots \\
\color{#fc6f86} a_{n1} & \color{#93f5eb} a_{n2}  &  \dots  &  \color{#d693f5} a_{nn}
\end{pmatrix}
$$
##### Example
Let $n=2$, a $2$x$2$ matrix gives us a pair of vectors in $\mathbb{R}^{2}$, which we visualize as a **Plane**. A vector in $\mathbb{R}^{2}$ is just a point in $\mathbb{R}^{2}$, but we represent it as an arrow from $0$ to the point.
$$
A = \begin{pmatrix}
 2  & 1 \\
1 & 2
\end{pmatrix}
\quad v_{1} = \begin{pmatrix}
2 \\
1
\end{pmatrix}
\quad v_{2} = \begin{pmatrix}
1 \\
2
\end{pmatrix}
$$

![[Drawing 2023-01-14 14.44.46.excalidraw.png | center | 256]]
Consider the parallelogram with vertices $0, v_{1},v_{2}, v_{1}+v_{2}$
**Area of Parallelogram** $=$ (length of a side) x (dist. from that side to the opposite side)
Let us define a function
	Area: $M_{2\text{x}2}(\mathbb{R}) \mapsto \mathbb{R}$
	$A = [v_{1},v_{2}] \mapsto$ area of parallelogram with vertices $0,v_{1},v_{2},v_{1}+v_{2}$
This function seems to be ***Linear*** in the variables $v_{1}$ and $v_{2}$

$Area(v_{1},\alpha v_{2}) = \alpha \cdot Area(v_{1},v_{2})$
![[Drawing 2023-01-14 14.44.46.excalidraw 1.png | center | 256]]

$Area(v_{1},v_{2}+v_{3}) = Area(v_{1},v_{2})+Area(v_{1},v_{3})$

![[Drawing 2023-01-14 14.44.46.excalidraw 2.png | center | 256]]

However, for the area to be genuinely linear in each variable, it cannot always be a positive number.
***Area will have a "sign".***
In other words, some pairs of vectors $v_{1},v_{2}$ will enclose parallelograms with ***negative area***.
In fact, for linearity, we need $Area(v_{1},-v_{2}) = -Area(v_{1},v_{2})$

![[Drawing 2023-01-14 14.44.46.excalidraw 3.png | center |  256]]

Also, for any $v$ we should have $Area(v,v)=0$
> The parallelogram with vertices $0,v,v,v+v$ is ***Flat***

So, for any vector $v$ and $w$ we have
	$Area(v+w,v+w)=0$

But,
$$
\begin{align}
Area(v+w,v+w) &= Area(v,v+w)+Area(w,v+w) \\
&= Area(v,v) + Area(v,w) \\
&\quad + Area(w,v) + Area(w,w) \\ \\
Area(v,v) &= 0 \:\:\: and \:\:\:Area(w,w) = 0 \\ \\
&\!\!\!\!\!\!\!\!\!\!\!\!Area(v,w) = -Area(w,v) 
\end{align}
$$

```ad-note
title: Good Area Function
Hence, An $Area$ function should satisfy:
1. $Area(v_{1}+v_{2},w) = Area(v_{1},w)+Area(v_{2},w)$
2. $Area(v,w_{1}+w_{2})=Area(v,w_{1}) + Area(v,w_{2})$
3. $Area(\alpha v,w) = \alpha \cdot Area(v,w)$

    $Area(v,\alpha w) = \alpha \cdot Area(v,w)$
4. $Area(v,w)=-Area(w,v)$
```


#### Higher Dimensions

Given an $n$x$n$ matrix, we view it as an $n$-tuple of column vectors $v_{1}\dots v_{n}$ . These vectors enclose an ***n-dimensional box*** with vertices $\alpha_{1}v_{1}+\alpha_{2}v_{2}+\dots+\alpha_{n}v_{n}$  where  $\alpha_{i}=0\:\:or\:\:1$

We want a **volume** function
	$Vol_{n}: M_{n\text{x}n}(\mathbb{R}) \rightarrow \mathbb{R}$
	$[v_{1},v_{2}\dots v_{n}] \longmapsto Vol_{n}(v_{1}, \dots v_{n})$
This function should be ***linear in each variable***

###### Note:
> If two vectors $v_{i},v_{j}$ interchange positions, the volume should change ***sign***


#### Unit of Measurement

An $n$-dimensional cube with each side equal to $1$ has volume $1$.
So if,
$$
\begin{align}
e_{i} = \begin{pmatrix}
0 \\
\vdots \\
1 \\
\vdots \\
0
\end{pmatrix}
\qquad 
\text{1 in row i and 0 in other rows}
\end{align}
$$
Then, $Vol_{n}(e_{1},e_{2}\dots e_{n}) = 1$

The volume function is called the **determinant**.
For any $n$, we have the function $\det: M_{n\text{x}n}(\mathbb{R}) \rightarrow \mathbb{R}$ 

